{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.encodeproject.org/search/?type=Experiment&control_type!=*&status=released&perturbed=false&assay_slims=DNA+accessibility&assay_title=DNase-seq&replicates.library.biosample.donor.organism.scientific_name=Homo+sapiens&files.platform.term_name=Illumina+HiSeq+4000&files.run_type=paired-ended&replicates.library.nucleic_acid_term_name=DNA'\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "encode_auth = ('3G5L4K3W',  'yqz7n22xndk5udyp')\n",
    "HEADERS = {'accept': 'application/json'}\n",
    "\n",
    "\n",
    "json_results = requests.get(url, headers=HEADERS, \n",
    "                            auth=encode_auth).json()\n",
    "\n",
    "experiments = [results['accession'] for results in json_results['@graph']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "        \n",
    "encode_auth = ('3G5L4K3W',  'yqz7n22xndk5udyp')\n",
    "HEADERS = {'accept': 'application/json'}\n",
    "\n",
    "\n",
    "ChipReplicationQualityMetrics = {\n",
    "    'reproducible_peaks', 'idr_cutoff', 'rescue_ratio', \n",
    "    'self_consistency_ratio', 'reproducibility'}\n",
    "\n",
    "ChipPeakEnrichmentQualityMetrics = {\n",
    "    'frip', 'min_size', '25_pct', '50_pct', '75_pct', 'max_size', 'mean'}\n",
    "\n",
    "ChipAlignmentEnrichmentQualityMetrics = {\n",
    "    'subsampled_reads', 'estimated_fragment_len', 'corr_estimated_fragment_len',\n",
    "    'phantom_peak', 'corr_phantom_peak', 'argmin_corr', 'min_corr', 'NSC', \n",
    "    'RSC', 'auc', 'syn_auc', 'x_intercept', 'syn_x_intercept', 'elbow_pt', \n",
    "    'syn_elbow_pt', 'jsd', 'syn_jsd', 'pct_genome_enrich', 'diff_enrich', \n",
    "    'ch_div'}  \n",
    "\n",
    "ChipLibraryQualityMetrics = {\n",
    "    'unpaired_reads', 'paired_reads', 'unmapped_reads', \n",
    "    'unpaired_duplicate_reads', 'paired_duplicate_reads',\n",
    "    'paired_optical_duplicate_reads', 'pct_duplicate_reads', 'total_fragments', \n",
    "    'distinct_fragments', 'positions_with_one_read', 'NRF', 'PBC1', 'PBC2'}\n",
    "\n",
    "ChipAlignmentQualityMetrics = {\n",
    "    'processing_stage', 'total_reads', 'total_reads_qc_failed', \n",
    "    'duplicate_reads', 'duplicate_reads_qc_failed', 'mapped_reads', \n",
    "    'mapped_reads_qc_failed', 'pct_mapped_reads', 'paired_reads', \n",
    "    'paired_reads_qc_failed', 'read1', 'read1_qc_failed', 'read2', \n",
    "    'read2_qc_failed', 'properly_paired_reads', \n",
    "    'properly_paired_reads_qc_failed', 'pct_properly_paired_reads', \n",
    "    'with_itself', 'with_itself_qc_failed', 'singletons', \n",
    "    'singletons_qc_failed', 'pct_singletons', 'diff_chroms', \n",
    "    'diff_chroms_qc_failed'}\n",
    "\n",
    "\n",
    "rows = []\n",
    "    \n",
    "\n",
    "def get_metadata_for_experiment(exp_accession_id):\n",
    "    \"\"\"Returns how many numbers lie within `maximum` and `minimum` in a given `row`\"\"\"\n",
    "    accession_id = 'NNNN'\n",
    "\n",
    "    search_url = \"https://www.encodeproject.org/{}\".format(\n",
    "        exp_accession_id)\n",
    "    json_results = requests.get(search_url, headers=HEADERS, \n",
    "                                auth=encode_auth).json()\n",
    "\n",
    "    assembly = ''\n",
    "\n",
    "    fastqs = []\n",
    "    fastqs_run_type = []\n",
    "    assay_type = []\n",
    "\n",
    "    alignments = [] \n",
    "    alignments_download_urls = []\n",
    "    alignments_md5sums = []\n",
    "    alignments_qc_metrics = []\n",
    "\n",
    "    unfiltered_alignments = []\n",
    "    unfiltered_alignments_download_urls = []\n",
    "    unfiltered_alignments_md5sums = []\n",
    "    unfiltered_alignments_qc_metrics = []\n",
    "\n",
    "    preferred_default_bed_narrowPeak = []\n",
    "    preferred_default_bed_narrowPeak_download_urls = []\n",
    "    preferred_default_bed_narrowPeak_md5sums = []\n",
    "    preferred_default_bed_narrowPeak_qc_metrics = []\n",
    "    \n",
    "    number_of_peaks = []\n",
    "\n",
    "    encode4_files = []\n",
    "    \n",
    "    try:\n",
    "        json_results['analyses']\n",
    "        \n",
    "    except:\n",
    "        print(f\"une problem with for {exp_accession_id}\")\n",
    "        return\n",
    "        \n",
    "            \n",
    "    for analysis_object in json_results['analyses']:\n",
    "        if 'ENCODE4' in analysis_object['pipeline_award_rfas']:\n",
    "            encode4_files = [f.split('/')[2] for f in \\\n",
    "                             analysis_object['files']]\n",
    "\n",
    "    #print(\"ENCODE4 files\", encode4_files)\n",
    "\n",
    "    for file_dict in json_results['files']:\n",
    "        if file_dict['file_type'] == 'fastq': \n",
    "            fastqs.append(file_dict['accession'])\n",
    "            fastqs_run_type.append(file_dict['run_type'])\n",
    "            continue\n",
    "\n",
    "        if file_dict['accession'] not in encode4_files:\n",
    "            #print(\"Skipping\", file_dict['accession'])\n",
    "            continue\n",
    "\n",
    "        # fastq files dont have 'assembly' so we ^^^ check\n",
    "        # that first\n",
    "        # this check may not be necessary because we are \n",
    "        # checking for ENCODE4\n",
    "        if file_dict['status'] == 'archived' or \\\n",
    "            file_dict['assembly'] == 'hg19':\n",
    "            continue\n",
    "\n",
    "        # this will get overwritten several times for each \n",
    "        # file, but it's ok \n",
    "        assembly = file_dict['assembly']\n",
    "\n",
    "        if file_dict['file_type'] == 'bed narrowPeak' and \\\n",
    "            'preferred_default' in file_dict.keys() and \\\n",
    "            file_dict['preferred_default']:\n",
    "\n",
    "            download_url = \"https://www.encodeproject.org/\" + \\\n",
    "               \"files/{}/@@download/{}.bed.gz\".format(\n",
    "                   file_dict['accession'], \n",
    "                   file_dict['accession'])\n",
    "\n",
    "            preferred_default_bed_narrowPeak.append(\n",
    "                file_dict['accession'])\n",
    "            preferred_default_bed_narrowPeak_download_urls.append(\n",
    "                download_url)\n",
    "            preferred_default_bed_narrowPeak_md5sums.append(\n",
    "                file_dict['md5sum'])\n",
    "\n",
    "            _qc_metrics = {}\n",
    "            for qc_metrics in file_dict['quality_metrics']:\n",
    "                if qc_metrics['@type'][0] == \\\n",
    "                    'ChipReplicationQualityMetric':\n",
    "                    _qc_metrics['ChipReplicationQualityMetric'] = \\\n",
    "                        {k: qc_metrics[k] for k in \\\n",
    "                         qc_metrics.keys() & \\\n",
    "                         ChipReplicationQualityMetrics}\n",
    "\n",
    "                if qc_metrics['@type'][0] == \\\n",
    "                    'ChipPeakEnrichmentQualityMetric':\n",
    "                    _qc_metrics['ChipPeakEnrichmentQualityMetric'] = \\\n",
    "                        {k: qc_metrics[k] for k in \\\n",
    "                         qc_metrics.keys() & \\\n",
    "                         ChipPeakEnrichmentQualityMetrics}\n",
    "\n",
    "            preferred_default_bed_narrowPeak_qc_metrics.append(\n",
    "                _qc_metrics)\n",
    "            \n",
    "#             if 'reproducible_peaks' in file_dict['quality_metrics'][0]:\n",
    "#                 number_of_peaks.append(file_dict['quality_metrics'][0]['reproducible_peaks'])\n",
    "#             else:\n",
    "#                 number_of_peaks.append(file_dict['quality_metrics'][1]['reproducible_peaks'])\n",
    "            \n",
    "            continue\n",
    "\n",
    "        if file_dict['file_type'] == 'bam': \n",
    "\n",
    "            download_url = \"https://www.encodeproject.org/\" + \\\n",
    "                           \"files/{}/@@download/{}.bam\".format(\n",
    "                               file_dict['accession'], \n",
    "                               file_dict['accession'])\n",
    "\n",
    "            _qc_metrics = {}\n",
    "            for qc_metrics in file_dict['quality_metrics']:\n",
    "                if qc_metrics['@type'][0] == \\\n",
    "                    'ChipAlignmentEnrichmentQualityMetric':\n",
    "                    _qc_metrics['ChipAlignmentEnrichmentQualityMetric'] = \\\n",
    "                        {k: qc_metrics[k] for k in \\\n",
    "                         qc_metrics.keys() & \\\n",
    "                         ChipAlignmentEnrichmentQualityMetrics}\n",
    "\n",
    "                if qc_metrics['@type'][0] == \\\n",
    "                    'ChipLibraryQualityMetric':\n",
    "                    _qc_metrics['ChipLibraryQualityMetric'] = \\\n",
    "                        {k: qc_metrics[k] for k in \\\n",
    "                         qc_metrics.keys() & \\\n",
    "                         ChipLibraryQualityMetrics}\n",
    "\n",
    "                if qc_metrics['@type'][0] == \\\n",
    "                    'ChipAlignmentQualityMetric':\n",
    "                    _qc_metrics['ChipAlignmentQualityMetric'] = \\\n",
    "                        {k: qc_metrics[k] for k in \\\n",
    "                         qc_metrics.keys() & \\\n",
    "                         ChipAlignmentQualityMetrics}\n",
    "\n",
    "            assay_type=file_dict[\"assay_title\"]\n",
    "            if file_dict['output_type'] == 'unfiltered alignments':\n",
    "                unfiltered_alignments.append(file_dict['accession'])  \n",
    "                unfiltered_alignments_download_urls.append(download_url)\n",
    "                unfiltered_alignments_md5sums.append(file_dict['md5sum'])\n",
    "                unfiltered_alignments_qc_metrics.append(_qc_metrics)\n",
    "\n",
    "            if file_dict['output_type'] == 'alignments':\n",
    "                alignments.append(file_dict['accession'])  \n",
    "                alignments_download_urls.append(download_url)\n",
    "                alignments_md5sums.append(file_dict['md5sum'])\n",
    "                alignments_qc_metrics.append(_qc_metrics)    \n",
    "\n",
    "    row = [accession_id, assay_type, exp_accession_id, fastqs, fastqs_run_type,\n",
    "           assembly, preferred_default_bed_narrowPeak, \n",
    "           preferred_default_bed_narrowPeak_download_urls, \n",
    "           preferred_default_bed_narrowPeak_md5sums, \n",
    "           preferred_default_bed_narrowPeak_qc_metrics,\n",
    "           unfiltered_alignments, unfiltered_alignments_download_urls,\n",
    "           unfiltered_alignments_md5sums, \n",
    "           unfiltered_alignments_qc_metrics, alignments, \n",
    "           alignments_download_urls, alignments_md5sums, \n",
    "           alignments_qc_metrics]\n",
    "\n",
    "    #print(row)\n",
    "\n",
    "    unique_run_types = set(fastqs_run_type)\n",
    "    if len(unique_run_types) > 1:\n",
    "        print(accession_id, exp_accession_id, \"run_types\", \n",
    "              len(unique_run_types), \"****** MULTIPLE RUN TYPES ******\")\n",
    "        \n",
    "    return (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "# Step 1: Init multiprocessing.Pool()\n",
    "pool = mp.Pool(40)\n",
    "\n",
    "# Step 2: `pool.apply` the `howmany_within_range()`\n",
    "results = pool.map(get_metadata_for_experiment, experiments)\n",
    "\n",
    "# Step 3: Don't forget to close\n",
    "pool.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results = [i for i in results if type(i)==type([])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(filtered_results) \n",
    "columns = ['experiment_series','assay_type' ,'experiment', 'fastqs', 'fastqs_run_type', \n",
    "       'assembly', 'preferred_default_bed_narrowPeak', \n",
    "       'preferred_default_bed_narrowPeak_download_urls', \n",
    "       'preferred_default_bed_narrowPeak_md5sums',\n",
    "       'preferred_default_bed_narrowPeak_qc_metrics', \n",
    "       'unfiltered_alignments', 'unfiltered_alignments_download_urls', \n",
    "       'unfiltered_alignments_md5sums', 'unfiltered_alignments_qc_metrics',\n",
    "       'alignments', 'alignments_download_urls', 'alignments_md5sums', \n",
    "       'alignments_qc_metrics']\n",
    "df.to_csv('metadata.tsv', \n",
    "           sep='\\t', index=False, header=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('metadata.tsv', \n",
    "                            sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments_urls = []\n",
    "alignments_md5sums = []\n",
    "unfiltered_alignments_urls = []\n",
    "unfiltered_alignments_md5sums = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    unfiltered_alignments_urls.extend(row['unfiltered_alignments_download_urls'])\n",
    "    unfiltered_alignments_md5sums.extend(row['unfiltered_alignments_md5sums'])\n",
    "    \n",
    "    alignments_urls.extend(row['alignments_download_urls'])\n",
    "    alignments_md5sums.extend(row['alignments_md5sums'])\n",
    "    \n",
    "\n",
    "alignments_urls_md5sums = ([list(a) for a in zip(alignments_urls, alignments_md5sums)])\n",
    "alignments_urls_md5sums_df = pd.DataFrame(alignments_urls_md5sums)\n",
    "alignments_urls_md5sums_df.to_csv('alignments_bams_urls_md5sums.txt', header=None, index=False, sep=' ')\n",
    "\n",
    "unfiltered_alignments_urls_md5sums = ([list(a) for a in zip(unfiltered_alignments_urls, unfiltered_alignments_md5sums)])\n",
    "unfiltered_alignments_urls_md5sums_df = pd.DataFrame(unfiltered_alignments_urls_md5sums)\n",
    "unfiltered_alignments_urls_md5sums_df.to_csv('unfiltered_alignments_bams_urls_md5sums.txt', header=None, index=False, sep=' ')\n",
    "\n",
    "# preferred default IDR thresholded peaks\n",
    "preferred_default_urls = []\n",
    "preferred_default_md5sums = []\n",
    "for idx, row in df.iterrows():\n",
    "    preferred_default_urls.extend(row['preferred_default_bed_narrowPeak_download_urls'])\n",
    "    preferred_default_md5sums.extend(row['preferred_default_bed_narrowPeak_md5sums'])\n",
    "\n",
    "preferred_default_urls_md5sums = ([list(a) for a in zip(preferred_default_urls, preferred_default_md5sums)])\n",
    "\n",
    "preferred_default_urls_md5sums_df = pd.DataFrame(preferred_default_urls_md5sums)\n",
    "preferred_default_urls_md5sums_df.to_csv('preferred_default_urls_md5sums.txt', header=None, index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
